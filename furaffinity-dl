#!/bin/bash
set -e

if [ "$1" = "" ] || [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
    echo "Usage: $0 SECTION/USER [YOUR_USERNAME]
Downloads the entire gallery/scraps/favorites of any furaffinity.net user.

Examples:
 $0 gallery/kodardragon
 $0 scraps/---
 $0 favorites/kivuli

You can also log in to FurAffinity and download restricted content, like this:
 $0 gallery/mithril07 your_username_here"
    exit 1
fi

runtime_dir="$HOME"'/.cache/furaffinity-dl'
# umask makes any new files created only readable by the owner.
# It is necessary to store cookies securely
(umask u=rwx,g=,o= && mkdir -p "$runtime_dir")
tempfile="$(umask u=rwx,g=,o= && mktemp --tmpdir=$runtime_dir)"
cookies_file="$(umask u=rwx,g=,o= && mktemp --tmpdir=$runtime_dir)"

cleanup() {
    rm -f "$tempfile"
    rm -f "$cookies_file"
}
trap cleanup EXIT

LOGIN_NAME="$2"
if [ "$LOGIN_NAME" = "" ]; then
    # set a wget wrapper with custom user agent
    fwget() {
        wget --user-agent="Mozilla/5.0 furaffinity-dl (https://github.com/Shnatsel/furaffinity-dl)" $*
    }
else
    # prompt the user for their password
    read -r -s -e -p "Password for $LOGIN_NAME (will not be displayed): " PASSWORD

    # log in to FurAffinity
    wget --user-agent="Mozilla/5.0 furaffinity-dl (https://github.com/Shnatsel/furaffinity-dl)" \
    --referer="https://www.furaffinity.net/login/" \
    --save-cookies "$cookies_file" \
    --post-data "action=login&retard_protection=1&name=$LOGIN_NAME&pass=$PASSWORD&login=Login+to%C2%A0FurAffinity" \
    -O - "https://www.furaffinity.net/login/" | grep "erroneous username or password" && {
        echo "Wrong password for user $LOGIN_NAME. Aborted." >&2
        exit 1
    }

    # set a wget wrapper with custom user agent and cookies
    fwget() {
        wget --user-agent="Mozilla/5.0 furaffinity-dl (https://github.com/Shnatsel/furaffinity-dl)" \
        --load-cookies "$cookies_file" $*
    }
fi

base_url=https://www.furaffinity.net/"$1"

url="$base_url"
page_counter=1

# Iterate over the gallery pages with thumbnails and links to artwork view pages
while true; do
    fwget -O "$tempfile" "$url"
    grep -q -i "there are no submissions to list" "$tempfile" && break

    # Extract links to pages with individual artworks and iterate over them
    artwork_pages=$(grep '<a href="/view/' "$tempfile" | grep -E --only-matching '/view/[[:digit:]]+/')
    for page in $artwork_pages; do
        # Download the submission page
        fwget -O "$tempfile" 'https://www.furaffinity.net'"$page"

        if grep -q "System Message" "$tempfile"; then
            echo "WARNING: $page seems to be inaccessible, skipping."
            continue
        fi

        # Get the full size image URL.
        # This will be a facdn.net link, we have to use HTTP
        # to get around DPI-based page blocking in some countries.
        image_url='http:'$(grep -E --max-count=1 --only-matching '"[^"]+">Download[[:space:]]?</a>' "$tempfile" | cut -d '"' -f 2)

        # TODO: Get the submission title out of the page
        # this trick may come in handy for avoiding slashes in filenames:
        # | tr '/' 'âˆ•'

        wget "$image_url"
    done
    page_counter=$((page_counter + 1))
    url="$base_url"/"$page_counter"
done
